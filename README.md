# ğŸ· Somm.dev

> **AI Code Evaluation with Sommelier Sophistication**

Somm.dev is a Multi-Agentic AI code evaluation platform that brings the expertise of a master sommelier to your repositories. Six specialized AI agents analyze your code from every angleâ€”structure, quality, security, innovationâ€”and deliver a comprehensive evaluation report with actionable recommendations.

**Live:** https://www.somm.dev

---

## ğŸš€ Quick Start

```bash
# Clone repository
git clone https://github.com/yourusername/somm.dev.git
cd somm.dev

# Start backend
cd backend
source venv/bin/activate
uvicorn app.main:app --reload --port 8000

# Start frontend (new terminal)
cd frontend
npm install
npm run dev

# Open browser
open http://localhost:3000
```

---

## ğŸ“ Project Structure

```
somm.dev/
â”œâ”€â”€ frontend/               # Next.js 16 + TypeScript + Tailwind CSS
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/           # App Router pages
â”‚   â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â””â”€â”€ lib/           # Utilities
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ backend/                # Python FastAPI + LangGraph
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/routes/    # REST API endpoints
â”‚   â”‚   â”œâ”€â”€ graph/         # LangGraph evaluation pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ nodes/     # 6 Sommelier AI agents
â”‚   â”‚   â”‚   â”œâ”€â”€ state.py   # Graph state definition
â”‚   â”‚   â”‚   â””â”€â”€ graph.py   # Graph orchestration
â”‚   â”‚   â”œâ”€â”€ prompts/       # LangChain prompts
â”‚   â”‚   â”‚   â””â”€â”€ criteria/  # 4 evaluation criteria modes
â”‚   â”‚   â”œâ”€â”€ models/        # Pydantic models
â”‚   â”‚   â””â”€â”€ services/      # Business logic
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ docs/                   # Comprehensive documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md           # System architecture & LangGraph design
â”‚   â”œâ”€â”€ EVALUATION_PIPELINE.md    # Multi-agent evaluation flow
â”‚   â”œâ”€â”€ EVALUATION_CRITERIA.md    # 4 criteria modes (basic/hackathon/academic/custom)
â”‚   â”œâ”€â”€ IMPLEMENTATION_PLAN.md    # Step-by-step implementation guide
â”‚   â””â”€â”€ BRANDING_GUIDE.md         # Brand identity & design system
â”‚
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Technology Stack

### Frontend
- **Next.js 16** - React framework with App Router
- **React 19** - UI library
- **TypeScript** - Type safety
- **Tailwind CSS 4** - Utility-first styling
- **Lucide React** - Icon library

### Backend
- **Python 3.12+** - Language
- **FastAPI** - Web framework
- **LangChain** - LLM orchestration framework
- **LangGraph** - Multi-agent workflow orchestration
- **Gemini 3 Flash** - LLM via LangChain
- **MongoDB** - Database
- **SSE** - Real-time streaming

---

## ğŸ¯ The Six Sommeliers

Somm.dev uses **six specialized AI agents** built with LangGraph to evaluate your code:

| Sommelier | Role | Focus Area | Color | Technique |
|-----------|------|------------|-------|-----------|
| ğŸ›ï¸ **Marcel** | Cellar Master | Structure & Metrics | #8B7355 | Repository organization |
| ğŸ­ **Isabella** | Wine Critic | Code Quality | #C41E3A | Aesthetics & DX |
| ğŸ” **Heinrich** | Quality Inspector | Testing & Security | #2F4F4F | Risk assessment |
| ğŸŒ± **Sofia** | Vineyard Scout | Innovation & Tech | #DAA520 | Growth opportunities |
| ğŸ› ï¸ **Laurent** | Winemaker | Implementation | #228B22 | Code craftsmanship |
| ğŸ¯ **Jean-Pierre** | Master Sommelier | Final Synthesis | #4169E1 | Final verdict |

**Architecture:** All 5 sommeliers (Marcel, Isabella, Heinrich, Sofia, Laurent) run in parallel using LangGraph's fan-out pattern, then Jean-Pierre synthesizes their results.

---

## ğŸ“Š Evaluation Criteria

Somm.dev provides **6 evaluation modes**:

| Mode | Use Case | Description |
|------|----------|-------------|
| **six_sommeliers** | Default evaluation | 6 AI sommelier agents with parallel fan-out pattern |
| **grand_tasting** | Quick evaluation | P0 priority techniques only for faster results |
| **full_techniques** | Comprehensive evaluation | All 75 techniques across 8 categories with deep synthesis |
| **basic** | General code review | Code Quality (25%), Architecture (20%), Documentation (20%), Testing (20%), Security (15%) |
| **hackathon** | Gemini 3 Hackathon judging | Technical (40%), Innovation (30%), Impact (20%), Presentation (10%) |
| **academic** | Research projects | Novelty (25%), Methodology (25%), Reproducibility (20%), Documentation (20%), Impact (10%) |

---

## ğŸ† Scoring System

Somm.dev evaluates repositories on a **0-100 point scale**:

| Score | Badge | Description |
|-------|-------|-------------|
| 95-100 | ğŸ† **Legendary** | Exceptional quality |
| 90-94 | ğŸ¥‡ **Grand Cru** | Outstanding |
| 85-89 | ğŸ¥ˆ **Premier Cru** | Excellent |
| 80-84 | ğŸ¥‰ **Village** | Good |
| 70-79 | ğŸ… **Table** | Acceptable |
| 60-69 | ğŸ· **House Wine** | Light, enjoyable |
| <60 | âš ï¸ **Corked** | Below standards |

---

## ğŸ“¡ API Endpoints

### Core Evaluation APIs

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/evaluate` | POST | Start code evaluation |
| `/api/evaluate/{id}/stream` | GET | SSE stream for progress |
| `/api/evaluate/{id}/result` | GET | Get evaluation results |
| `/api/history` | GET | User evaluation history |

### Techniques API

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/techniques` | GET | List all 75 techniques (filter by category, hat, mode) |
| `/api/techniques/stats` | GET | Get aggregated statistics |
| `/api/techniques/{id}` | GET | Get detailed technique definition |

### Request Example
```bash
curl -X POST http://localhost:8000/api/evaluate \
  -H "Content-Type: application/json" \
  -d '{
    "repo_url": "https://github.com/user/repo",
    "criteria": "basic"
  }'
```

### Response Example
```json
{
  "evaluation_id": "eval_abc123",
  "status": "pending",
  "estimated_time": 30
}
```

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [ARCHITECTURE.md](./docs/ARCHITECTURE.md) | System architecture, LangGraph design patterns, data flow |
| [EVALUATION_PIPELINE.md](./docs/EVALUATION_PIPELINE.md) | Multi-agent evaluation workflow, node implementations |
| [EVALUATION_CRITERIA.md](./docs/EVALUATION_CRITERIA.md) | 4 evaluation modes with detailed criteria and prompts |
| [IMPLEMENTATION_PLAN.md](./docs/IMPLEMENTATION_PLAN.md) | Step-by-step implementation guide (10-day plan) |
| [BRANDING_GUIDE.md](./docs/BRANDING_GUIDE.md) | Brand identity, terminology, design system |

---

## ğŸ¨ Design System

### Colors
```css
--somm-burgundy: #722F37;      /* Primary */
--somm-champagne: #F7E7CE;     /* Secondary */
--somm-parchment: #FAF4E8;     /* Background */
--somm-cellar: #2E4A3F;        /* Accent */
```

### Typography
- **Headings:** Playfair Display (serif)
- **Body:** Inter (sans-serif)

---

## ğŸš¢ Deployment

### Backend (Fly.io/Railway)
```bash
cd backend
docker build -t somm-backend .
fly deploy
```

### Frontend (Vercel)
```bash
cd frontend
vercel --prod
```

---

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
pytest

# Frontend tests
cd frontend
npm test
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'feat: Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

---

## ğŸ“ License

MIT Â© 2025 Somm.dev Team

â€” The Somm.dev Team
